#!/usr/bin/env python



import argparse
import h5py
import os
import sys
import pandas as pd

from astropy import units, constants, table

from tardisatomic import sql_stmts
import numpy as np
import sqlite3
from collections import OrderedDict

import hashlib
import uuid

import pdb

try:
    import sqlparse
    sqlparse_available = True
except ImportError:
    sqlparse_available = False

parser = argparse.ArgumentParser()

parser.add_argument('hdf5', help='HDF5 file that is created')
parser.add_argument('atom_db', help='atom_db')
parser.add_argument('--kurucz_loggf_threshold', default=-3., help='log(gf) threshold for the kurucz dataset')
parser.add_argument('--metastable_loggf_threshold', default=-3., help='log(gf) threshold for flagging levels that are connected by lines metastable')
parser.add_argument('--include_zeta', action='store_true', default=False, help='include recombination coefficients (zeta)')
parser.add_argument('--include_collision', action='store_true', default=False, help='include collision rates')
parser.add_argument('--build_macro', action='store_true', default=False, help='build macro atom table into the file')
parser.add_argument('--max_atomic_number', default=30, help=('Setting the maximum atomic number to be stored in the HDF5'
            'file. For now 30 is the limit as I don\'t have reliable ionization data for anything above.'))
parser.add_argument('--synpp_refs', help='point to the synpp_refs.dat')
parser.add_argument('--include_ion_cx', action='store_true', default=False, help='include ionization cross sections (ion_cx)')

atom_group = parser.add_mutually_exclusive_group()


atom_group.add_argument('--exclude_atoms', nargs='+',
                        help='exclude atoms to be added to the HDF5 (specify atomic_number)')

atom_group.add_argument('--include_atoms', nargs='+',
                        help='include atoms to be added to the HDF5 (specify atomic_number)')

ion_group = parser.add_mutually_exclusive_group()

ion_group.add_argument('--exclude_ions', nargs='+',
                        help='exclude atoms to be added to the HDF5 (specify atomic_number)')

ion_group.add_argument('--include_ions', nargs='+',
                        help='include atoms to be added to the HDF5 (specify atomic_number)')





args = parser.parse_args()

if args.exclude_ions is not None:
    ion_filter_stmt = 'ion not in (%s)' % ', '.join(args.exclude_ions)

elif args.include_ions is not None:
    ion_filter_stmt = 'ion in (%s)' % ', '.join(args.include_ions)
else:
    ion_filter_stmt = ''

if args.exclude_atoms is not None:
    atom_filter_stmt = 'atom not in (%s)' % ', '.join(args.exclude_atoms)

elif args.include_atoms is not None:
    atom_filter_stmt = 'atom in (%s)' % ', '.join(args.include_atoms)
else:
    atom_filter_stmt = ''


basic_atom_data_fname = os.path.join(os.path.dirname(sql_stmts.__file__), 'data', 'atom_data_basic.h5')
zeta_datafile = os.path.join(os.path.dirname(sql_stmts.__file__), 'data', 'knox_long_recombination_zeta.dat')


####### Opening HDF5 file  ##########################
if os.path.exists(args.hdf5):
    print "HDF5 File exists - this procedure will overwrite it"
    if raw_input('Okay? [Y/n]').lower().startswith('n'):
        sys.exit(0)
os.system('cp %s %s' % (basic_atom_data_fname, args.hdf5))
hdf5_file = h5py.File(args.hdf5)
######################################################


ionization_data = pd.DataFrame(hdf5_file['ionization_data'].__array__())
ionization_data.set_index(['atomic_number', 'ion_number'], inplace=True)

if not os.path.exists(args.atom_db):
    raise ValueError('Database %s does not exist' % args.atom_db)

conn = sqlite3.connect(args.atom_db)

try:
    conn.execute('create index lines_h5_index on lines(atom, ion, level_id_lower, level_id_upper)')
except sqlite3.OperationalError as e:
    print e

lines_data_sql_stmt = ('select id, wl, atom, ion, f_ul, f_lu, loggf, level_id_lower, level_id_upper, source from lines')
#lines_data_sql_stmt += 'where  loggf > %s ' % args.loggf_threshold



where_stmt = ['atom <= %d' % args.max_atomic_number]

if ion_filter_stmt != '':
    where_stmt.append(ion_filter_stmt)
if atom_filter_stmt != '':
    where_stmt.append(atom_filter_stmt)

if where_stmt != []:
    lines_data_sql_stmt += ' where %s' % ' and '.join(where_stmt)

lines_data_sql_stmt += ' order by wl'


print lines_data_sql_stmt

lines_data_dtype = [('line_id', np.int), ('wavelength', np.float), ('atomic_number', np.int), ('ion_number', np.int), ('f_ul', np.float),
                   ('f_lu', np.float), ('loggf', np.float), ('level_number_lower', np.int), ('level_number_upper', np.int), ('source', '|S64')]


lines_data = conn.execute(lines_data_sql_stmt).fetchall()
lines_data = np.array(lines_data, dtype=lines_data_dtype)

lines_data = pd.DataFrame(lines_data)


einstein_coeff = (4 * np.pi ** 2 * constants.e.gauss.value ** 2) / (
    constants.m_e.cgs.value * constants.c.cgs.value)

lines_data['nu'] = units.Unit('angstrom').to('Hz', lines_data['wavelength'], units.spectral())
lines_data['B_lu'] = einstein_coeff * lines_data['f_lu']  / (constants.h.cgs.value * lines_data['nu'])
lines_data['B_ul'] = einstein_coeff * lines_data['f_ul'] / (constants.h.cgs.value * lines_data['nu'])
lines_data['A_ul'] = 2 * einstein_coeff * lines_data['nu'] ** 2 / constants.c.cgs.value ** 2 * lines_data['f_ul']



lines_data_level_number_upper = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_upper'])
lines_data_level_number_upper = lines_data_level_number_upper.groupby(level=['atomic_number', 'ion_number', 'level_number_upper'])

lines_data_level_number_lower = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_lower'])
lines_data_level_number_lower = lines_data_level_number_lower.groupby(level=['atomic_number', 'ion_number', 'level_number_lower'])

#lines_data = lines_data.set_index('line_id')



levels_sql_stmt = 'select atom, ion, level_id, energy, g, source from levels '

where_stmt = ['atom <= %d' % args.max_atomic_number]

if ion_filter_stmt != '':
    where_stmt.append(ion_filter_stmt)
if atom_filter_stmt != '':
    where_stmt.append(atom_filter_stmt)

if where_stmt != []:
    levels_sql_stmt += ' where %s' % ' and '.join(where_stmt)



levels_sql_stmt += ' ORDER BY atom,ion, level_id'



print levels_sql_stmt

levels_dtype = [('atomic_number', np.int), ('ion_number', np.int), ('level_number', np.int),
                ('energy', np.float), ('g', np.int), ('source', '|S64')]
#levels_units = ('1', '1', '1', 'eV', '1', '1')


levels_data = conn.execute(levels_sql_stmt).fetchall()
levels_data = np.array(levels_data, dtype=levels_dtype)
levels_data = pd.DataFrame(levels_data)
levels_data.set_index(['atomic_number', 'ion_number', 'level_number'], inplace=True)

print "Creating metastable flags"
lines_data_level_number_upper_meta = lines_data[lines_data.loggf>args.metastable_loggf_threshold]
lines_data_level_number_upper_meta = lines_data_level_number_upper_meta.groupby(['atomic_number', 'ion_number', 'level_number_upper'])

count_down = lines_data_level_number_upper_meta['line_id'].count()
levels_data['metastable'] = count_down == 0
levels_data['metastable'] = pd.isnull(levels_data['metastable'])


#culling autoionizing
levels_atom_ion_index = zip(levels_data.index.get_level_values('atomic_number'),
                            levels_data.index.get_level_values('ion_number').values + 1)

levels_ionization_energy = ionization_data.ix[levels_atom_ion_index]
levels_ionization_energy = levels_ionization_energy.values.reshape(levels_ionization_energy.shape[0])

auto_ionizing_levels_mask = (levels_data.energy >= levels_ionization_energy).values

auto_ionizing_levels = levels_data[auto_ionizing_levels_mask]

auto_ionizing_line_ids = []
#Culling lines_data with low gf values
lines_data = lines_data[(lines_data.source!='kurucz') | (lines_data.loggf > args.kurucz_loggf_threshold)]
print "Cleaning auto-ionizing"
for index in auto_ionizing_levels.index:
    index_in_lines = False
    try:
        line_group = lines_data_level_number_upper.get_group(index)
    except KeyError:
        pass
    else:
        auto_ionizing_line_ids += line_group.line_id.values.tolist()
        index_in_lines = True

    try:
        line_group = lines_data_level_number_lower.get_group(index)
    except KeyError:
        pass
    else:
        auto_ionizing_line_ids += line_group.line_id.values.tolist()
        index_in_lines = True

    if not index_in_lines:
        print "INDEX %s not in lines" % (index,)

auto_ionizing_line_ids = np.unique(auto_ionizing_line_ids)
lines_data = lines_data[~lines_data.reset_index().line_id.isin(auto_ionizing_line_ids).values]
levels_data = levels_data[~auto_ionizing_levels_mask]


print "cleaning levels which don't exist in lines"
existing_levels_ids = []
for index in levels_data.index:
    index_in_lines = False
    try:
        line_group = lines_data_level_number_upper.get_group(index)
    except KeyError:
        if index[0] == index[1]:
            print "Found special index %s" % (index,)
            index_in_lines = True
        pass
    else:
        index_in_lines = True

    try:
        line_group = lines_data_level_number_lower.get_group(index)
    except KeyError:
        if index[0] == index[1]:
            print "Found special index %s" % (index,)
            index_in_lines = True

        pass
    else:
        index_in_lines = True

    if not index_in_lines:
        pass
#        print "INDEX %s not in lines" % (index,)
    else:
        existing_levels_ids.append(index)

print "Found %d levels which don't exist in lines (total=%d levels)" % (len(levels_data)-len(existing_levels_ids), len(levels_data))
levels_data = levels_data.ix[existing_levels_ids]
levels_prev_index = pd.Series(index=levels_data.index.copy())
levels_data.reset_index(inplace=True).set_index(['atomic_number', 'ion_number'], inplace=True)

print "Relabeling index numbers"

for index in np.unique(levels_data.index.values):
    levels_prev_index.ix[index] = np.arange(levels_data.ix[index].level_number.count())
    levels_data.ix[index].level_number = np.arange(levels_data.ix[index].level_number.count())


lines_data_level_number_upper = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_upper'])
lines_data_level_number_upper_index = lines_data_level_number_upper.index.copy()
lines_data_level_number_upper = lines_data_level_number_upper.groupby(level=['atomic_number', 'ion_number', 'level_number_upper'])

lines_data_level_number_lower = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_lower'])
lines_data_level_number_lower_index  = lines_data_level_number_lower.index.copy()
lines_data_level_number_lower = lines_data_level_number_lower.groupby(level=['atomic_number', 'ion_number', 'level_number_lower'])


lines_data.level_number_lower = levels_prev_index.ix[lines_data_level_number_lower_index].values
lines_data.level_number_upper = levels_prev_index.ix[lines_data_level_number_upper_index].values


lines_data_level_number_upper = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_upper'])
lines_data_level_number_upper_index = lines_data_level_number_upper.index.copy()
lines_data_level_number_upper = lines_data_level_number_upper.groupby(level=['atomic_number', 'ion_number', 'level_number_upper'])

lines_data_level_number_lower = lines_data.set_index(['atomic_number', 'ion_number', 'level_number_lower'])
lines_data_level_number_lower_index  = lines_data_level_number_lower.index.copy()
lines_data_level_number_lower = lines_data_level_number_lower.groupby(level=['atomic_number', 'ion_number', 'level_number_lower'])

lines_data = lines_data.set_index('line_id')
levels_data = levels_data.reset_index().set_index(['atomic_number', 'ion_number', 'level_number'])

del lines_data['loggf']
del lines_data['source']
del levels_data['source']

hdf5_file['lines_data'] = lines_data.reset_index().to_records(index=False)
hdf5_file['levels_data'] = levels_data.reset_index().to_records(index=False)


#Culling all levels above the ionization threshold (auto-ionizing levels)

#load zeta
if args.include_zeta:
    zeta_data = np.loadtxt(zeta_datafile, usecols=xrange(1,23), dtype=np.float64)
    hdf5_file['zeta_data'] = zeta_data
    hdf5_file['zeta_data'].attrs['t_rad'] = np.arange(2000, 42000, 2000)
    hdf5_file['zeta_data'].attrs['source'] = 'Used with kind permission from Knox Long'


if args.build_macro:

    macro_atom = OrderedDict()
    macro_atom['atomic_number'] = []
    macro_atom['ion_number'] = []
    macro_atom['source_level_number'] = []
    macro_atom['destination_level_number'] = []
    macro_atom['transition_type'] = []
    macro_atom['transition_probability'] = []
    macro_atom['transition_line_id'] = []

    macro_atom_references = OrderedDict()
    macro_atom_references['atomic_number'] = []
    macro_atom_references['ion_number'] = []
    macro_atom_references['source_level_number'] = []
    macro_atom_references['count_down'] = []
    macro_atom_references['count_up'] = []
    macro_atom_references['count_total'] = []
    current_atomic_number = 0
    for index, (energy, g, metastable) in levels_data.iterrows():
        if current_atomic_number != index[0]:
            current_atomic_number = index[0]
            print "Starting Atom %d" % current_atomic_number

        try:
            down_transitions = lines_data_level_number_upper.get_group(index)
        except KeyError:
            down_level_count = 0
        else:
            macro_atom['atomic_number'] += down_transitions.index.get_level_values('atomic_number').tolist() * 2
            macro_atom['ion_number'] += down_transitions.index.get_level_values('ion_number').tolist() * 2
            macro_atom['source_level_number'] += down_transitions.index.get_level_values('level_number_upper').tolist() * 2
            macro_atom['destination_level_number'] += down_transitions.level_number_lower.tolist() * 2
            down_index = [(index[0], index[1], down_index) for down_index in down_transitions.level_number_lower.tolist()]
            down_level_count = down_transitions.level_number_lower.count()
            macro_atom['transition_type'] += [-1] * down_level_count
            macro_atom['transition_type'] += [0] * down_level_count
            nus = lines_data.nu.ix[down_transitions.line_id.values].values
            f_ul = lines_data.f_ul.ix[down_transitions.line_id.values].values
            e_lower = levels_data.ix[down_index]['energy'].values
            macro_atom['transition_probability'] += list(2 * nus**2 * f_ul / constants.c.cgs.value**2 * (energy - e_lower))
            macro_atom['transition_probability'] += list(2 * nus**2 * f_ul / constants.c.cgs.value**2 * e_lower)

            macro_atom['transition_line_id'] += down_transitions.line_id.tolist() * 2

        try:
            up_transitions = lines_data_level_number_lower.get_group(index)
        except KeyError:
            up_level_count = 0
        else:
            macro_atom['atomic_number'] += up_transitions.index.get_level_values('atomic_number').tolist()
            macro_atom['ion_number'] += up_transitions.index.get_level_values('ion_number').tolist()
            macro_atom['source_level_number'] += up_transitions.index.get_level_values('level_number_lower').tolist()
            macro_atom['destination_level_number'] += up_transitions.level_number_upper.tolist()
            up_level_count = up_transitions.level_number_upper.count()
            macro_atom['transition_type'] += [1] * up_level_count

            nus = lines_data.nu.ix[up_transitions.line_id.values].values
            f_lu = lines_data.f_lu.ix[up_transitions.line_id.values].values

            macro_atom['transition_probability'] += list(f_lu * energy / (constants.h.cgs.value * nus))

            macro_atom['transition_line_id'] += up_transitions.line_id.tolist()

        macro_atom_references['atomic_number'].append(index[0])
        macro_atom_references['ion_number'].append(index[1])
        macro_atom_references['source_level_number'].append(index[2])
        macro_atom_references['count_down'].append(down_level_count)
        macro_atom_references['count_up'].append(up_level_count)
        macro_atom_references['count_total'].append(2*down_level_count + up_level_count)

    hdf5_file['macro_atom_data'] = pd.DataFrame(macro_atom).to_records(index=False)
    hdf5_file['macro_atom_references'] = pd.DataFrame(macro_atom_references).to_records(index=False)




if args.include_collision:

    collision_data_exists = conn.execute('select count(name) from sqlite_master where name="collision_data"').fetchone()[0]

    if collision_data_exists == 0:
        print "WARNING: Collision data requested but not in the database"
    elif collision_data_exists == 1:
        collision_columns = zip(*conn.execute('pragma table_info(collision_data)').fetchall())[1]

        temperature_columns = [item for item in collision_columns if item.startswith('t')]
        temperatures = [float(item[1:]) for item in temperature_columns]
        select_collision_stmt = "select atom, ion, level_number_upper, level_number_lower, g_ratio, delta_e, %s" \
                                " from collision_data"

        select_collision_stmt = select_collision_stmt % (', '.join(temperature_columns))

        collision_data_dtype = [('atomic_number', np.int), ('ion_number', np.int), ('level_number_upper', np.int),
                                ('level_number_lower', np.int), ('g_ratio', np.float), ('delta_e', np.float)]

        collision_data_dtype += [(str(item), np.float) for item in temperature_columns]

        collision_data = conn.execute(select_collision_stmt).fetchall()
        collision_data = np.array(collision_data, dtype=collision_data_dtype)

        if len(collision_data) != 0:
            hdf5_file['collision_data'] = collision_data
            hdf5_file['collision_data'].attrs['temperatures'] = temperatures


if args.synpp_refs is not None:
    synpp_refs_data = np.genfromtxt(args.synpp_refs, usecols=(0,1), dtype=[int, float])
    synpp_atom = np.floor(synpp_refs_data['f0']/100.).astype(int)
    synpp_ion = (synpp_refs_data['f0'] - synpp_atom*100)
    synpp_line = synpp_refs_data['f1']

    synpp_line_ids = []
    all_species = np.unique(lines_data.set_index(['atomic_number', 'ion_number']).index.values).tolist()
    for atom, ion, wl in zip(synpp_atom, synpp_ion, synpp_line):
        if (atom, ion) not in all_species:
            print "Species %s found in synpp but not in current atom set" % ((atom, ion), )
            continue
        current_lines_data = lines_data[(lines_data['atomic_number']==atom) & (lines_data['ion_number'] == ion)]
        if current_lines_data.empty:
            raise ValueError('Fix incomplete linelist later')
        synpp_line_ids.append(current_lines_data.index[np.argmin(np.abs(current_lines_data['wavelength'] - wl))])

    synpp_ref = np.array(zip(synpp_atom, synpp_ion, synpp_line, synpp_line_ids),
                         dtype=[('atomic_number', np.int64), ('ion_number', np.int64), ('wavelength', np.float64), ('line_id', np.int64)])

    hdf5_file['synpp_refs'] = synpp_ref

if args.include_ion_cx:
    select_ion_cx_th = 'SELECT levels.atom, levels.ion, levels.level_id, ion_cx.cx_threshold FROM levels, ion_cx WHERE levels.level_id = ion_cx.level_id AND ion_cx.atom = levels.atom AND ion_cx.ion = levels.ion'
    select_ion_cx_sp = 'SELECT levels.atom, levels.ion, levels.level_id, ion_cx.cx_threshold, ion_cx_supporter.nu, ion_cx_supporter.xs  FROM ion_cx_supporter INNER JOIN ion_cx ON ion_cx_supporter.ion_cx_id = ion_cx.id INNER JOIN levels ON ion_cx.level_id = levels.level_id AND ion_cx.atom = levels.atom AND ion_cx.ion = levels.ion'
    ion_cx_th_dtype = [('atomic_number',np.int),('ion_number',np.int),('level_id',np.int),('ion_cx_threshold',np.float)]
    ion_cx_sp_dtype = [('atomic_number',np.int),('ion_number',np.int),('level_id',np.int),('ion_cx_threshold',np.float),('nu',np.float),('xs',np.float)]
    ion_cx_th_data = conn.execute(select_ion_cx_th).fetchall()
    ion_cx_th_data = np.array(ion_cx_th_data, ion_cx_th_dtype)
    ion_cx_sp_data = conn.execute(select_ion_cx_sp).fetchall()
    ion_cx_sp_data = np.array(ion_cx_sp_data, ion_cx_sp_dtype)
    hdf5_file['ionization_cx_threshold'] = ion_cx_th_data
    hdf5_file['ionization_cx_support'] = ion_cx_sp_data






print "Signing Atom_data with MD5 and UUID1"

md5_hash = hashlib.md5()
for dataset in hdf5_file.values():
    md5_hash.update(dataset.value.data)




uuid1 = uuid.uuid1().hex

hdf5_file.attrs['md5'] = md5_hash.hexdigest()
hdf5_file.attrs['uuid1'] = uuid1

hdf5_file.close()





