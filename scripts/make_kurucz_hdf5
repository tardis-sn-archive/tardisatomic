#!/usr/bin/env python



import argparse
import h5py
import os

from astropy import units, constants, table

from tardisatomic import sql_stmts
import numpy as np
import sqlite3
from collections import OrderedDict
import pdb

try:
    import sqlparse
    sqlparse_available = True
except ImportError:
    sqlparse_available = False

parser = argparse.ArgumentParser()

parser.add_argument('hdf5', help='HDF5 file that is created')
parser.add_argument('atom_db', help='atom_db')
parser.add_argument('--loggf_threshold', default=-3., help='log(gf) threshold')
parser.add_argument('--include_zeta', action='store_true', default=False, help='include recombination coefficients (zeta)')

parser.add_argument('--build_macro', action='store_true', default=False, help='build macro atom table into the file')
atom_group = parser.add_mutually_exclusive_group()


atom_group.add_argument('--exclude_atoms', nargs='+',
                        help='exclude atoms to be added to the HDF5 (specify atomic_number)')

atom_group.add_argument('--include_atoms', nargs='+',
                        help='include atoms to be added to the HDF5 (specify atomic_number)')

ion_group = parser.add_mutually_exclusive_group()

ion_group.add_argument('--exclude_ions', nargs='+',
                        help='exclude atoms to be added to the HDF5 (specify atomic_number)')

ion_group.add_argument('--include_ions', nargs='+',
                        help='include atoms to be added to the HDF5 (specify atomic_number)')





args = parser.parse_args()

if args.exclude_ions is not None:
    ion_filter_stmt = 'ion not in (%s)' % ', '.join(args.exclude_ions)

elif args.include_ions is not None:
    ion_filter_stmt = 'ion in (%s)' % ', '.join(args.include_ions)
else:
    ion_filter_stmt = ''

if args.exclude_atoms is not None:
    atom_filter_stmt = 'atom not in (%s)' % ', '.join(args.exclude_atoms)

elif args.include_atoms is not None:
    atom_filter_stmt = 'atom in (%s)' % ', '.join(args.include_atoms)
else:
    atom_filter_stmt = ''


basic_atom_data_fname = os.path.join(os.path.dirname(sql_stmts.__file__), 'data', 'atom_data_basic.h5')
zeta_datafile = os.path.join(os.path.dirname(sql_stmts.__file__), 'data', 'knox_long_recombination_zeta.dat')


os.system('cp %s %s' % (basic_atom_data_fname, args.hdf5))
hdf5_file = h5py.File(args.hdf5)

conn = sqlite3.connect(args.atom_db)

line_list_sql_stmt = ('select id, wl, atom, ion, f_ul, f_lu, level_id_lower, level_id_upper from lines')
#line_list_sql_stmt += 'where  loggf > %s ' % args.loggf_threshold


where_stmt = ['loggf > %s' % args.loggf_threshold]

if ion_filter_stmt != '':
    where_stmt.append(ion_filter_stmt)
if atom_filter_stmt != '':
    where_stmt.append(atom_filter_stmt)

if where_stmt != []:
    line_list_sql_stmt += ' where %s' % ' and '.join(where_stmt)


print line_list_sql_stmt

line_list_dtype = [('line_id', np.int), ('wavelength', np.float), ('atomic_number', np.int), ('ion_number', np.int), ('f_ul', np.float),
                   ('f_lu', np.float), ('level_number_lower', np.int), ('level_number_upper', np.int)]

line_list_units = ('1', 'angstrom', '1', '1', '1', '1', '1', '1')
line_list = conn.execute(line_list_sql_stmt).fetchall()
line_list = np.array(line_list, dtype=line_list_dtype)

hdf5_file['lines_data'] = line_list
hdf5_file['lines_data'].attrs['units'] = line_list_units

levels_sql_stmt = 'select atom, ion, level_id, energy, g, metastable from levels '

where_stmt = []

if ion_filter_stmt != '':
    where_stmt.append(ion_filter_stmt)
if atom_filter_stmt != '':
    where_stmt.append(atom_filter_stmt)

if where_stmt != []:
    levels_sql_stmt += ' where %s' % ' and '.join(where_stmt)



levels_sql_stmt += ' ORDER BY atom,ion,energy,g'



print levels_sql_stmt

levels_dtype = [('atomic_number', np.int), ('ion_number', np.int), ('level_number', np.int),
                ('energy', np.float), ('g', np.int), ('metastable', np.bool)]
levels_units = ('1', '1', '1', 'eV', '1', '1')
print(levels_sql_stmt)
print(levels_dtype)
levels_list = conn.execute(levels_sql_stmt).fetchall()
levels_list = np.array(levels_list, dtype=levels_dtype)

hdf5_file['levels_data'] = levels_list
hdf5_file['levels_data'].attrs['units'] = levels_units


#load zeta
if args.include_zeta:
    zeta_data = np.loadtxt(zeta_datafile, usecols=xrange(1,23), dtype=np.float64)
    hdf5_file['zeta_data'] = zeta_data
    hdf5_file['zeta_data'].attrs['t_rad'] = np.arange(2000, 42000, 2000)
    hdf5_file['zeta_data'].attrs['source'] = 'Used with kind permission from Knox Long'


if args.build_macro:
    probability_set = []

    nus = units.Unit('angstrom').to('Hz', line_list['wavelength'], units.spectral())


    level_ids = []

    macro_select_levels = """select
                                id, wl, ?, f_lu, f_ul, level_id_{destination_relation}
                            from
                                lines

                            where
                             atom=? and ion=? and level_id_{source_relation}=?"""

    macro_select_energy = """select
                                energy, g
                            from
                                levels
                            where
                                atom=?
                                and ion=?
                                and level_id=?"""


    level_id_dtype = [('line_id', np.int), ('wavelength', np.float), ('f_lu', np.float), ('f_ul', np.float), ('source_level_id', np.int), ('destination_level_id', np.int)]

    macro_atom = OrderedDict()
    macro_atom['atomic_number'] = []
    macro_atom['ion_number'] = []
    macro_atom['source_level_number'] = []
    macro_atom['destination_level_number'] = []
    macro_atom['transition_type'] = []
    macro_atom['transition_probability'] = []
    macro_atom['transition_line_id'] = []

    macro_atom_counts = OrderedDict()
    macro_atom_counts['atomic_number'] = []
    macro_atom_counts['ion_number'] = []
    macro_atom_counts['source_level_number'] = []
    macro_atom_counts['count_down'] = []
    macro_atom_counts['count_up'] = []
    macro_atom_counts['count_total'] = []

    last_atom = -1
    for atom, ion, level_id, energy, g in conn.execute('select atom, ion, level_id, energy, g from levels'):
        if last_atom != atom:
            print "new atom %d" % atom


        macro_atom_counts['atomic_number'].append(atom)
        macro_atom_counts['ion_number'].append(ion)

        last_atom = atom
        #p_down ->



        macro_select_levels_fmt = macro_select_levels.format(source_relation='upper', destination_relation='lower')
        level_id_wl = conn.execute(macro_select_levels_fmt, (level_id, atom, ion, level_id)).fetchall()
        level_id_wl = np.array(level_id_wl, dtype=level_id_dtype)
        nu = units.Unit('Angstrom').to('Hz', level_id_wl['wavelength'], units.spectral())

        level_energy_g = [conn.execute(macro_select_energy, (atom, ion, int(cur_level_id))).fetchone() for cur_level_id in level_id_wl['destination_level_id']]
        level_energy_g = np.array(level_energy_g, dtype=[('energy', np.float), ('g', np.float)])

        p_emission_down = 2*nu**2*(level_energy_g['g'] / g) * level_id_wl['f_lu'] * units.Unit('eV').to('erg', (energy - level_energy_g['energy']))
        p_internal_down = 2*nu**2*(level_energy_g['g'] / g) * level_id_wl['f_lu'] * units.Unit('eV').to('erg', level_energy_g['energy'])


        macro_atom_counts['source_level_number'].append(level_id)
        macro_atom_counts['count_down'].append(len(nu))
        macro_atom['atomic_number'] += [atom]*len(nu)*2
        macro_atom['ion_number'] += [ion]*len(nu)*2
        macro_atom['source_level_number']+= [level_id]*len(nu)*2
        #for p_emission_down and p_internal_down
        macro_atom['destination_level_number']+= list(level_id_wl['destination_level_id'])*2



        macro_atom['transition_type'] += [-1]*len(nu)
        macro_atom['transition_type'] += [0]*len(nu)

        macro_atom['transition_probability'] += list(p_emission_down)
        macro_atom['transition_probability'] += list(p_internal_down)

        macro_atom['transition_line_id'] += list(level_id_wl['line_id'])*2


        macro_select_levels_fmt = macro_select_levels.format(source_relation='lower', destination_relation='upper')

        level_id_wl = conn.execute(macro_select_levels_fmt, (level_id, atom, ion, level_id)).fetchall()
        level_id_wl = np.array(level_id_wl, dtype=level_id_dtype)
        nu = units.Unit('Angstrom').to('Hz', level_id_wl['wavelength'], units.spectral())

        level_energy_g = [conn.execute(macro_select_energy, (atom, ion, int(cur_level_id))).fetchone() for cur_level_id in level_id_wl['destination_level_id']]
        level_energy_g = np.array(level_energy_g, dtype=[('energy', np.float), ('g', np.float)])

        p_internal_up = 1/(constants.cgs.h.value*nu) * level_id_wl['f_lu']
        macro_atom_counts['count_up'].append(len(nu))
        macro_atom['atomic_number'] += [atom]*len(nu)
        macro_atom['ion_number'] += [ion]*len(nu)
        macro_atom['source_level_number']+= [level_id]*len(nu)
        #for p_emission_down and p_internal_down
        macro_atom['destination_level_number']+= list(level_id_wl['destination_level_id'])
        macro_atom['transition_type'] += [1]*len(nu)
        macro_atom['transition_probability'] += list(p_internal_up)

        macro_atom['transition_line_id'] += list(level_id_wl['line_id'])


        macro_atom_counts['count_total'].append(2*macro_atom_counts['count_down'][-1] + macro_atom_counts['count_up'][-1])

    macro_atom_table = table.Table(macro_atom)

    macro_atom_counts_table = table.Table(macro_atom_counts)

    hdf5_file['macro_atom_data'] = macro_atom_table.__array__()
    hdf5_file['macro_atom_references'] = macro_atom_counts_table.__array__()


hdf5_file.close()
